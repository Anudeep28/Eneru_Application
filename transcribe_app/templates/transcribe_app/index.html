{% extends "ocr_app/base.html" %}

{% block extra_css %}
<style>
    .recording {
        animation: pulse 1.5s infinite;
    }
    @keyframes pulse {
        0% { transform: scale(1); }
        50% { transform: scale(1.1); }
        100% { transform: scale(1); }
    }
    .markdown-preview {
        font-family: 'Segoe UI', system-ui, -apple-system, sans-serif;
        line-height: 1.6;
        padding: 1rem;
    }
    .btn {
        font-weight: bold;
        padding: 0.5rem 1rem;
        border-radius: 0.25rem;
        cursor: pointer;
        transition: background-color 0.3s ease;
    }
    .btn-primary {
        background-color: #3b82f6;
        color: white;
        border: none;
    }
    .btn-primary:hover {
        background-color: #2563eb;
    }
    .btn-secondary {
        background-color: #6b7280;
        color: white;
        border: none;
    }
    .btn-secondary:hover {
        background-color: #4b5563;
    }
    .btn-danger {
        background-color: #ef4444;
        color: white;
        border: none;
    }
    .btn-danger:hover {
        background-color: #dc2626;
    }
</style>
{% endblock %}

{% block content %}
<div class="container mx-auto px-4 py-8">
    <div class="max-w-3xl mx-auto">
        {% csrf_token %}
        
        <!-- Recording Controls -->
        <div class="flex justify-center gap-4 mb-8">
            <button id="recordButton" 
                    class="bg-blue-500 hover:bg-blue-700 text-white font-bold py-2 px-4 rounded focus:outline-none focus:shadow-outline"
                    onclick="handleRecordButtonClick()">
                Start Recording
            </button>
        </div>
        
        <!-- Recording Status -->
        <div class="text-center mb-8">
            <span id="recordingStatus" class="text-lg font-medium text-gray-700">Click 'Start Recording' to begin</span>
            <div id="errorMessage" class="text-red-500 mt-2 hidden"></div>
        </div>
        
        <!-- Transcription Output -->
        <div class="mb-8">
            <textarea id="transcriptionText" 
                      class="w-full h-64 p-4 border rounded-lg resize-none shadow-sm"
                      readonly
                      placeholder="Transcription will appear here..."></textarea>
        </div>
        
        <!-- Download Options -->
        <div id="downloadOptions" class="mt-8 flex justify-center space-x-4">
            <button id="downloadMarkdown" 
                    class="bg-green-600 text-white px-6 py-3 rounded-lg hover:bg-green-700 transition-colors duration-300 flex items-center space-x-2 opacity-50 cursor-not-allowed"
                    disabled>
                <i class="fas fa-file-alt"></i>
                <span>Download Markdown</span>
            </button>
            <button id="downloadWord" 
                    class="bg-blue-600 text-white px-6 py-3 rounded-lg hover:bg-blue-700 transition-colors duration-300 flex items-center space-x-2 opacity-50 cursor-not-allowed"
                    disabled>
                <i class="fas fa-file-word"></i>
                <span>Download Word</span>
            </button>
        </div>
    </div>
</div>
{% endblock %}

{% block extra_js %}
<script>
// Global transcription manager instance
let transcriptionManager = null;

// Get CSRF token
function getCSRFToken() {
    const csrfInput = document.querySelector('input[name="csrfmiddlewaretoken"]');
    return csrfInput ? csrfInput.value : '';
}

// Initialize when the DOM is fully loaded
document.addEventListener('DOMContentLoaded', async function() {
    console.log('Initializing transcription manager...');
    try {
        transcriptionManager = new AudioTranscriptionManager();
        console.log('Transcription manager initialized');
    } catch (error) {
        console.error('Initialization error:', error);
    }
});

class AudioTranscriptionManager {
    constructor() {
        console.log('Constructing AudioTranscriptionManager');
        this.WAV_SAMPLE_RATE = 16000;
        this.CHUNK_DURATION = 30; // 30 seconds chunk
        this.SILENCE_THRESHOLD = 0.01; // Threshold for silence detection
        this.MIN_SILENCE_DURATION = 1; // Minimum silence duration in seconds to trigger chunk processing
        
        this.audioContext = null;
        this.mediaRecorder = null;
        this.currentStream = null;
        this.audioChunks = [];
        this.processingChunks = []; // Separate array for processing
        this.isRecording = false;
        this.isProcessing = false;
        this.accumulatedText = '';
        this.transcriptionQueue = [];
        this.isProcessingQueue = false;
        this.chunkStartTime = 0;
        this.lastAudioLevel = 0;
        this.silenceStartTime = 0;
        this.totalSamples = 0;

        // Initialize UI elements
        this.recordButton = document.getElementById('recordButton');
        this.transcriptionText = document.getElementById('transcriptionText');
        this.recordingStatus = document.getElementById('recordingStatus');
        
        // Initialize audio context with specific sample rate
        this.audioContext = new (window.AudioContext || window.webkitAudioContext)({
            sampleRate: this.WAV_SAMPLE_RATE
        });

        // Bind methods
        this.processNextInQueue = this.processNextInQueue.bind(this);
    }

    async processCurrentChunk() {
        if (this.audioChunks.length === 0) return;

        try {
            // Calculate total samples needed for 30 seconds
            const samplesNeeded = this.WAV_SAMPLE_RATE * 30;
            
            // Create a copy of the chunks for processing
            this.processingChunks = [...this.audioChunks];
            
            // Calculate total length
            const totalLength = this.processingChunks.reduce((acc, chunk) => acc + chunk.length, 0);
            
            // If we have more than 30 seconds worth of audio, we'll process multiple chunks
            while (this.processingChunks.length > 0) {
                let currentChunkLength = 0;
                let chunksToProcess = [];
                
                // Collect chunks until we have 30 seconds or run out of chunks
                while (currentChunkLength < samplesNeeded && this.processingChunks.length > 0) {
                    const chunk = this.processingChunks.shift();
                    chunksToProcess.push(chunk);
                    currentChunkLength += chunk.length;
                }
                
                // Concatenate the chunks we're going to process
                const concatenated = new Float32Array(currentChunkLength);
                let offset = 0;
                for (const chunk of chunksToProcess) {
                    concatenated.set(chunk, offset);
                    offset += chunk.length;
                }
                
                // Pad or trim to exactly 30 seconds
                let processedAudio;
                if (concatenated.length < samplesNeeded) {
                    // Pad with silence if less than 30 seconds
                    processedAudio = new Float32Array(samplesNeeded);
                    processedAudio.set(concatenated, 0);
                } else {
                    // Take exactly 30 seconds
                    processedAudio = concatenated.slice(0, samplesNeeded);
                    
                    // If we have remaining samples, put them back at the start of processingChunks
                    if (concatenated.length > samplesNeeded) {
                        this.processingChunks.unshift(concatenated.slice(samplesNeeded));
                    }
                }

                // Convert to WAV and add to queue
                const wavBlob = await this.createWavBlob(processedAudio);
                this.addToTranscriptionQueue(wavBlob);
            }
            
            // Clear processed chunks from the main audioChunks array
            const remainingSamples = totalLength % samplesNeeded;
            if (remainingSamples > 0) {
                // Keep only the remainder that wasn't processed
                this.audioChunks = [this.audioChunks[this.audioChunks.length - 1].slice(-remainingSamples)];
            } else {
                this.audioChunks = [];
            }

        } catch (error) {
            console.error('Error processing audio chunks:', error);
            this.showError('Error processing audio. Please try again.');
        }
    }

    async startRecording() {
        try {
            console.log('Starting recording...');
            if (this.isRecording) {
                console.warn('Already recording');
                return;
            }

            // Reset recording state
            this.accumulatedText = '';
            this.transcriptionText.value = '';
            this.transcriptionQueue = [];
            this.isProcessingQueue = false;
            this.audioChunks = [];
            this.processingChunks = [];
            this.chunkStartTime = this.audioContext.currentTime;
            this.silenceStartTime = 0;
            this.lastAudioLevel = 0;
            this.totalSamples = 0;

            // Request microphone access with specific constraints
            this.currentStream = await navigator.mediaDevices.getUserMedia({ 
                audio: {
                    sampleRate: this.WAV_SAMPLE_RATE,
                    channelCount: 1,
                    echoCancellation: true,
                    noiseSuppression: true,
                    autoGainControl: true
                }
            });
            
            // Create audio processing pipeline
            const source = this.audioContext.createMediaStreamSource(this.currentStream);
            const processor = this.audioContext.createScriptProcessor(4096, 1, 1);
            
            processor.onaudioprocess = (e) => {
                if (this.isRecording) {
                    const inputData = e.inputBuffer.getChannelData(0);
                    this.audioChunks.push(new Float32Array(inputData));
                    this.totalSamples += inputData.length;
                    
                    // Calculate current audio level
                    const audioLevel = Math.sqrt(inputData.reduce((acc, val) => acc + val * val, 0) / inputData.length);
                    
                    const currentTime = this.audioContext.currentTime;
                    const samplesFor30Seconds = this.WAV_SAMPLE_RATE * 30;

                    // Check for silence
                    if (audioLevel < this.SILENCE_THRESHOLD) {
                        if (this.silenceStartTime === 0) {
                            this.silenceStartTime = currentTime;
                        } else if (currentTime - this.silenceStartTime >= this.MIN_SILENCE_DURATION) {
                            if (this.totalSamples >= this.WAV_SAMPLE_RATE) { // At least 1 second of audio
                                console.log('Silence detected, processing chunk...');
                                this.processCurrentChunk();
                                this.silenceStartTime = 0;
                            }
                        }
                    } else {
                        this.silenceStartTime = 0;
                    }

                    // Process chunk if we have accumulated 30 seconds of audio
                    if (this.totalSamples >= samplesFor30Seconds) {
                        console.log('30-second duration reached, processing chunk...');
                        this.processCurrentChunk();
                        this.totalSamples = this.totalSamples % samplesFor30Seconds; // Keep track of remaining samples
                    }

                    this.lastAudioLevel = audioLevel;
                }
            };
            
            source.connect(processor);
            processor.connect(this.audioContext.destination);
            
            this.isRecording = true;
            this.recordButton.textContent = 'Stop Recording';
            this.recordingStatus.textContent = 'Recording in progress...';
            this.recordingStatus.classList.add('text-red-500');

        } catch (error) {
            console.error('Error starting recording:', error);
            this.showError('Could not start recording. Please check microphone permissions.');
        }
    }

    async stopRecording() {
        if (!this.isRecording) return;
        
        console.log('Stopping recording...');
        this.isRecording = false;
        if (this.currentStream) {
            this.currentStream.getTracks().forEach(track => track.stop());
        }

        // Process any remaining audio only if it's substantial
        if (this.audioChunks.length > 0) {
            await this.processCurrentChunk();
        }

        // Clear the transcription queue and reset processing state
        this.transcriptionQueue = [];
        this.isProcessingQueue = false;
        this.audioChunks = [];
        this.processingChunks = [];
        this.totalSamples = 0;

        this.recordButton.textContent = 'Start Recording';
        this.recordingStatus.textContent = 'Recording stopped.';
        this.recordingStatus.classList.remove('text-red-500');
    }

    async processNextInQueue() {
        if (this.transcriptionQueue.length === 0 || !this.isRecording) {
            this.isProcessingQueue = false;
            return;
        }

        this.isProcessingQueue = true;
        const wavBlob = this.transcriptionQueue.shift();

        try {
            const reader = new FileReader();
            reader.readAsDataURL(wavBlob);
            
            reader.onloadend = async () => {
                // If recording was stopped while processing, don't continue
                if (!this.isRecording) {
                    this.isProcessingQueue = false;
                    this.transcriptionQueue = [];
                    return;
                }

                const base64data = reader.result.split(',')[1];
                
                try {
                    console.log('Sending audio chunk for transcription...');
                    const response = await fetch('/transcribe/transcribe_audio/', {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json',
                            'X-CSRFToken': getCSRFToken()
                        },
                        body: JSON.stringify({
                            audio: base64data,
                            sample_rate: this.WAV_SAMPLE_RATE
                        })
                    });

                    if (!response.ok) {
                        throw new Error(`Transcription failed: ${await response.text()}`);
                    }

                    // Only process response if still recording
                    if (this.isRecording) {
                        const result = await response.json();
                        console.log('Received transcription result:', result);
                        
                        if (result.success && result.text) {
                            if (this.accumulatedText) {
                                this.accumulatedText += ' ' + result.text;
                            } else {
                                this.accumulatedText = result.text;
                            }
                            this.transcriptionText.value = this.accumulatedText;
                            this.transcriptionText.scrollTop = this.transcriptionText.scrollHeight;
                        }
                    }
                } catch (error) {
                    console.error('Transcription error:', error);
                    this.showError('Failed to transcribe audio chunk.');
                }

                // Only process next chunk if still recording
                if (this.isRecording) {
                    this.processNextInQueue();
                } else {
                    this.isProcessingQueue = false;
                    this.transcriptionQueue = [];
                }
            };
        } catch (error) {
            console.error('Error processing audio:', error);
            this.isProcessingQueue = false;
        }
    }

    async createWavBlob(audioData) {
        // Convert to 16-bit PCM
        const pcmData = new Int16Array(audioData.length);
        for (let i = 0; i < audioData.length; i++) {
            const s = Math.max(-1, Math.min(1, audioData[i]));
            pcmData[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
        }

        // Create WAV header
        const wavHeader = new ArrayBuffer(44);
        const view = new DataView(wavHeader);
        
        // "RIFF" chunk descriptor
        writeString(view, 0, 'RIFF');
        view.setUint32(4, 36 + pcmData.length * 2, true);
        writeString(view, 8, 'WAVE');
        
        // "fmt " sub-chunk
        writeString(view, 12, 'fmt ');
        view.setUint32(16, 16, true);
        view.setUint16(20, 1, true);
        view.setUint16(22, 1, true);
        view.setUint32(24, this.WAV_SAMPLE_RATE, true);
        view.setUint32(28, this.WAV_SAMPLE_RATE * 2, true);
        view.setUint16(32, 2, true);
        view.setUint16(34, 16, true);
        
        // "data" sub-chunk
        writeString(view, 36, 'data');
        view.setUint32(40, pcmData.length * 2, true);

        return new Blob([wavHeader, pcmData.buffer], { type: 'audio/wav' });
    }

    addToTranscriptionQueue(wavBlob) {
        this.transcriptionQueue.push(wavBlob);
        if (!this.isProcessingQueue) {
            this.processNextInQueue();
        }
    }

    showError(message) {
        const errorDiv = document.getElementById('errorMessage');
        errorDiv.textContent = message;
        errorDiv.classList.remove('hidden');
        setTimeout(() => {
            errorDiv.classList.add('hidden');
        }, 5000);
    }
}

// Handle record button click
async function handleRecordButtonClick() {
    console.log('Record button clicked');
    if (!transcriptionManager) {
        console.error('Transcription manager not initialized');
        return;
    }

    if (transcriptionManager.isRecording) {
        await transcriptionManager.stopRecording();
    } else {
        await transcriptionManager.startRecording();
    }
}

// Download buttons event listeners
document.getElementById('downloadMarkdown').addEventListener('click', async () => {
    const text = document.getElementById('transcriptionText').value;
    if (!text) return;

    try {
        const response = await fetch('/transcribe/download/markdown/', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'X-CSRFToken': getCSRFToken(),
            },
            body: JSON.stringify({ text })
        });
        
        if (response.ok) {
            const blob = await response.blob();
            const url = window.URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = 'transcription.md';
            document.body.appendChild(a);
            a.click();
            window.URL.revokeObjectURL(url);
            document.body.removeChild(a);
        }
    } catch (error) {
        console.error('Error downloading markdown:', error);
    }
});

document.getElementById('downloadWord').addEventListener('click', async () => {
    const text = document.getElementById('transcriptionText').value;
    if (!text) return;

    try {
        const response = await fetch('/transcribe/download/word/', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'X-CSRFToken': getCSRFToken(),
            },
            body: JSON.stringify({ text })
        });
        
        if (response.ok) {
            const blob = await response.blob();
            const url = window.URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = 'transcription.docx';
            document.body.appendChild(a);
            a.click();
            window.URL.revokeObjectURL(url);
            document.body.removeChild(a);
        }
    } catch (error) {
        console.error('Error downloading word document:', error);
    }
});

// Helper function to write string to DataView
function writeString(view, offset, string) {
    for (let i = 0; i < string.length; i++) {
        view.setUint8(offset + i, string.charCodeAt(i));
    }
}
</script>
{% endblock %}
