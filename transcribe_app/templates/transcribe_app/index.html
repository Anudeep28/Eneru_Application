{% extends "ocr_app/base.html" %}

{% block extra_css %}
<style>
    .recording {
        animation: pulse 1.5s infinite;
    }
    @keyframes pulse {
        0% { transform: scale(1); }
        50% { transform: scale(1.1); }
        100% { transform: scale(1); }
    }
    .markdown-preview {
        font-family: 'Segoe UI', system-ui, -apple-system, sans-serif;
        line-height: 1.6;
        padding: 1rem;
    }
    .btn {
        font-weight: bold;
        padding: 0.5rem 1rem;
        border-radius: 0.25rem;
        cursor: pointer;
        transition: background-color 0.3s ease;
    }
    .btn-primary {
        background-color: #3b82f6;
        color: white;
        border: none;
    }
    .btn-primary:hover {
        background-color: #2563eb;
    }
    .btn-secondary {
        background-color: #6b7280;
        color: white;
        border: none;
    }
    .btn-secondary:hover {
        background-color: #4b5563;
    }
    .btn-danger {
        background-color: #ef4444;
        color: white;
        border: none;
    }
    .btn-danger:hover {
        background-color: #dc2626;
    }
</style>
{% endblock %}

{% block content %}
<div class="container mx-auto px-4 py-8">
    <div class="max-w-3xl mx-auto">
        {% csrf_token %}
        
        <!-- Recording Controls -->
        <div class="flex justify-center gap-4 mb-8">
            <button id="recordButton" 
                    class="bg-blue-500 hover:bg-blue-700 text-white font-bold py-2 px-4 rounded focus:outline-none focus:shadow-outline"
                    onclick="handleRecordButtonClick()">
                Start Recording
            </button>
            <button id="resumeButton" 
                    class="bg-green-500 hover:bg-green-700 text-white font-bold py-2 px-4 rounded focus:outline-none focus:shadow-outline hidden"
                    onclick="handleResumeButtonClick()">
                Resume Recording
            </button>
            <button id="clearButton" 
                    class="bg-red-500 hover:bg-red-700 text-white font-bold py-2 px-4 rounded focus:outline-none focus:shadow-outline hidden"
                    onclick="handleClearButtonClick()">
                Clear
            </button>
        </div>
        
        <!-- Recording Status -->
        <div class="text-center mb-8">
            <span id="recordingStatus" class="text-lg font-medium text-gray-700">Click 'Start Recording' to begin</span>
            <div id="errorMessage" class="text-red-500 mt-2 hidden"></div>
        </div>
        
        <!-- Transcription Output -->
        <div class="mb-8">
            <textarea id="transcriptionText" 
                      class="w-full h-64 p-4 border rounded-lg resize-none shadow-sm"
                      readonly
                      placeholder="Transcription will appear here..."></textarea>
        </div>
        
        <!-- Download Options -->
        <div id="downloadOptions" class="mt-8 flex justify-center space-x-4">
            <button id="downloadMarkdown" 
                    class="bg-green-600 text-white px-6 py-3 rounded-lg hover:bg-green-700 transition-colors duration-300 flex items-center space-x-2 opacity-50 cursor-not-allowed"
                    disabled>
                <i class="fas fa-file-alt"></i>
                <span>Download Markdown</span>
            </button>
            <button id="downloadWord" 
                    class="bg-blue-600 text-white px-6 py-3 rounded-lg hover:bg-blue-700 transition-colors duration-300 flex items-center space-x-2 opacity-50 cursor-not-allowed"
                    disabled>
                <i class="fas fa-file-word"></i>
                <span>Download Word</span>
            </button>
        </div>
    </div>
</div>
{% endblock %}

{% block extra_js %}
<script>
// Global transcription manager instance
let transcriptionManager = null;

// Get CSRF token
function getCSRFToken() {
    const csrfInput = document.querySelector('input[name="csrfmiddlewaretoken"]');
    return csrfInput ? csrfInput.value : '';
}

// Initialize when the DOM is fully loaded
document.addEventListener('DOMContentLoaded', async function() {
    console.log('Initializing transcription manager...');
    try {
        transcriptionManager = new AudioTranscriptionManager();
        console.log('Transcription manager initialized');
    } catch (error) {
        console.error('Initialization error:', error);
    }
});

class AudioTranscriptionManager {
    constructor() {
        console.log('Constructing AudioTranscriptionManager');
        this.WAV_SAMPLE_RATE = 16000;
        this.CHUNK_DURATION = 30; // 30 seconds chunk
        this.SILENCE_THRESHOLD = 0.005; // Reduced threshold for silence detection
        this.MIN_SILENCE_DURATION = 1; // Minimum silence duration in seconds to trigger chunk processing
        this.AUTO_STOP_SILENCE_DURATION = 1; // Duration of silence before auto-stop
        this.MIN_RECORDING_DURATION = 2; // Minimum recording duration before allowing auto-stop
        
        this.audioContext = null;
        this.mediaRecorder = null;
        this.currentStream = null;
        this.audioChunks = [];
        this.processingChunks = []; // Separate array for processing
        this.isRecording = false;
        this.isProcessing = false;
        this.accumulatedText = '';
        this.temporaryText = ''; // Store text between pauses
        this.transcriptionQueue = [];
        this.isProcessingQueue = false;
        this.chunkStartTime = 0;
        this.lastAudioLevel = 0;
        this.silenceStartTime = 0;
        this.totalSamples = 0;
        this.autoStopTimeout = null;
        this.isAutoStopped = false;
        this.recordingStartTime = 0;
        this.consecutiveSilenceCount = 0;
        this.processor = null;
        this.source = null;

        // Initialize UI elements
        this.recordButton = document.getElementById('recordButton');
        this.resumeButton = document.getElementById('resumeButton');
        this.transcriptionText = document.getElementById('transcriptionText');
        this.recordingStatus = document.getElementById('recordingStatus');
        this.downloadMarkdown = document.getElementById('downloadMarkdown');
        this.downloadWord = document.getElementById('downloadWord');
        this.clearButton = document.getElementById('clearButton');
        
        // Bind methods
        this.processNextInQueue = this.processNextInQueue.bind(this);
        this.handleSilence = this.handleSilence.bind(this);
    }

    async initializeAudioContext() {
        if (this.audioContext && this.audioContext.state !== 'closed') {
            await this.audioContext.close();
        }
        this.audioContext = new (window.AudioContext || window.webkitAudioContext)({
            sampleRate: this.WAV_SAMPLE_RATE
        });
    }

    async startRecording() {
        try {
            console.log('Starting recording...');
            if (this.isRecording) {
                console.warn('Already recording');
                return;
            }

            // Initialize audio context
            await this.initializeAudioContext();

            // Reset recording state
            this.accumulatedText = '';
            this.transcriptionText.value = this.temporaryText; // Show any previous text
            this.transcriptionQueue = [];
            this.isProcessingQueue = false;
            this.audioChunks = [];
            this.processingChunks = [];
            this.chunkStartTime = this.audioContext.currentTime;
            this.silenceStartTime = 0;
            this.lastAudioLevel = 0;
            this.totalSamples = 0;
            this.isAutoStopped = false;
            this.recordingStartTime = Date.now();
            this.consecutiveSilenceCount = 0;

            // Reset UI
            this.resumeButton.classList.add('hidden');
            this.downloadMarkdown.disabled = true;
            this.downloadWord.disabled = true;
            this.downloadMarkdown.classList.add('opacity-50', 'cursor-not-allowed');
            this.downloadWord.classList.add('opacity-50', 'cursor-not-allowed');

            // Request microphone access with specific constraints
            this.currentStream = await navigator.mediaDevices.getUserMedia({ 
                audio: {
                    sampleRate: this.WAV_SAMPLE_RATE,
                    channelCount: 1,
                    echoCancellation: true,
                    noiseSuppression: true,
                    autoGainControl: true
                }
            });
            
            // Create audio processing pipeline
            this.source = this.audioContext.createMediaStreamSource(this.currentStream);
            this.processor = this.audioContext.createScriptProcessor(4096, 1, 1);
            
            this.processor.onaudioprocess = (e) => {
                if (this.isRecording) {
                    const inputData = e.inputBuffer.getChannelData(0);
                    this.audioChunks.push(new Float32Array(inputData));
                    this.totalSamples += inputData.length;
                    
                    // Calculate current audio level
                    const audioLevel = Math.sqrt(inputData.reduce((acc, val) => acc + val * val, 0) / inputData.length);
                    
                    const currentTime = this.audioContext.currentTime;
                    const samplesFor30Seconds = this.WAV_SAMPLE_RATE * 30;
                    const recordingDuration = (Date.now() - this.recordingStartTime) / 1000;

                    // Only check for silence after minimum recording duration
                    if (recordingDuration >= this.MIN_RECORDING_DURATION) {
                        // Check for silence
                        if (audioLevel < this.SILENCE_THRESHOLD) {
                            this.consecutiveSilenceCount++;
                            if (this.silenceStartTime === 0) {
                                this.silenceStartTime = currentTime;
                                this.handleSilence();
                            }
                        } else {
                            this.consecutiveSilenceCount = 0;
                            this.silenceStartTime = 0;
                            if (this.autoStopTimeout) {
                                clearTimeout(this.autoStopTimeout);
                                this.autoStopTimeout = null;
                            }
                        }
                    }

                    // Process chunk if we have accumulated 30 seconds of audio
                    if (this.totalSamples >= samplesFor30Seconds) {
                        console.log('30-second duration reached, processing chunk...');
                        this.processCurrentChunk();
                        this.totalSamples = this.totalSamples % samplesFor30Seconds;
                    }

                    this.lastAudioLevel = audioLevel;
                }
            };
            
            this.source.connect(this.processor);
            this.processor.connect(this.audioContext.destination);
            
            this.isRecording = true;
            this.recordButton.textContent = 'Stop Recording';
            this.recordingStatus.textContent = 'Recording in progress...';
            this.recordingStatus.classList.add('text-red-500');

        } catch (error) {
            console.error('Error starting recording:', error);
            this.showError('Could not start recording. Please check microphone permissions.');
        }
    }

    handleSilence() {
        if (this.autoStopTimeout) {
            clearTimeout(this.autoStopTimeout);
        }

        // Only auto-stop if we have enough consecutive silence frames
        if (this.consecutiveSilenceCount >= 10) { // About 0.5 seconds of consecutive silence
            this.autoStopTimeout = setTimeout(async () => {
                if (this.isRecording && !this.isAutoStopped) {
                    console.log('Auto-stopping due to silence');
                    this.isAutoStopped = true;
                    
                    // Process any remaining audio
                    if (this.audioChunks.length > 0) {
                        await this.processCurrentChunk(true);
                    }

                    // Store current text temporarily
                    this.temporaryText = this.transcriptionText.value;
                    
                    // Stop recording but keep the text
                    await this.stopRecording(true);
                    
                    // Show resume button
                    this.resumeButton.classList.remove('hidden');
                    this.recordButton.classList.add('hidden');
                    this.recordingStatus.textContent = 'Recording paused due to silence. Click Resume to continue.';
                    this.recordingStatus.classList.remove('text-red-500');
                }
            }, this.AUTO_STOP_SILENCE_DURATION * 1000);
        }
    }

    async stopRecording(isAutoStop = false) {
        console.log('Stopping recording...');
        if (!this.isRecording) {
            console.warn('Not recording');
            return;
        }

        try {
            // Process any remaining audio immediately
            if (this.audioChunks.length > 0) {
                console.log('Processing remaining audio chunks before stopping...');
                await this.processCurrentChunk(true); // Pass true to indicate final chunk
            }

            // Clean up recording state
            this.isRecording = false;
            if (this.currentStream) {
                this.currentStream.getTracks().forEach(track => track.stop());
            }
            this.currentStream = null;

            // Update UI based on stop type
            if (!isAutoStop) {
                this.recordButton.textContent = 'Start Recording';
                this.recordingStatus.textContent = 'Recording stopped.';
                this.recordingStatus.classList.remove('text-red-500');
                this.downloadMarkdown.disabled = false;
                this.downloadWord.disabled = false;
                this.downloadMarkdown.classList.remove('opacity-50', 'cursor-not-allowed');
                this.downloadWord.classList.remove('opacity-50', 'cursor-not-allowed');
                this.resumeButton.classList.add('hidden');
                this.recordButton.classList.remove('hidden');
                this.clearButton.classList.remove('hidden');
            }

        } catch (error) {
            console.error('Error stopping recording:', error);
            this.showError('Error stopping recording');
        }
    }

    async processCurrentChunk(isFinalChunk = false) {
        if (!this.audioChunks.length) return;

        console.log('Processing audio chunk...');
        const currentChunks = [...this.audioChunks];
        this.audioChunks = []; // Reset for next chunk
        this.totalSamples = 0;
        this.chunkStartTime = this.audioContext.currentTime;

        try {
            // Combine all audio chunks into a single buffer
            const totalLength = currentChunks.reduce((acc, chunk) => acc + chunk.length, 0);
            const combinedBuffer = new Float32Array(totalLength);
            let offset = 0;
            for (const chunk of currentChunks) {
                combinedBuffer.set(chunk, offset);
                offset += chunk.length;
            }

            // Create WAV file
            const wavBuffer = this.createWAVFile(combinedBuffer);
            const wavBlob = new Blob([wavBuffer], { type: 'audio/wav' });

            // Add to processing queue
            this.transcriptionQueue.push({
                blob: wavBlob,
                isFinalChunk: isFinalChunk
            });

            // Start processing if not already processing
            if (!this.isProcessingQueue) {
                await this.processNextInQueue();
            }

        } catch (error) {
            console.error('Error processing audio chunk:', error);
            this.showError('Error processing audio');
        }
    }

    async processNextInQueue() {
        if (this.isProcessingQueue || this.transcriptionQueue.length === 0) return;

        this.isProcessingQueue = true;
        const { blob, isFinalChunk } = this.transcriptionQueue.shift();

        try {
            const formData = new FormData();
            formData.append('audio', blob, 'recording.wav');
            formData.append('is_final_chunk', isFinalChunk);

            const response = await fetch('/transcribe/transcribe_audio/', {
                method: 'POST',
                body: formData,
                headers: {
                    'X-CSRFToken': this.getCsrfToken()
                }
            });

            if (!response.ok) throw new Error('Transcription failed');

            const result = await response.json();
            if (result.text) {
                if (isFinalChunk) {
                    // For final chunk, append to accumulated text and update display
                    this.accumulatedText += ' ' + result.text;
                    this.transcriptionText.value = this.accumulatedText.trim();
                } else {
                    // For regular chunks during recording
                    this.accumulatedText += ' ' + result.text;
                    this.transcriptionText.value = this.accumulatedText.trim();
                }
            }

        } catch (error) {
            console.error('Error in transcription:', error);
            this.showError('Transcription failed');
        } finally {
            this.isProcessingQueue = false;
            // Process next chunk if any
            if (this.transcriptionQueue.length > 0) {
                await this.processNextInQueue();
            }
        }
    }

    createWAVFile(audioData) {
        // Convert to 16-bit PCM
        const pcmData = new Int16Array(audioData.length);
        for (let i = 0; i < audioData.length; i++) {
            const s = Math.max(-1, Math.min(1, audioData[i]));
            pcmData[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
        }

        // Create WAV header
        const wavHeader = new ArrayBuffer(44);
        const view = new DataView(wavHeader);
        
        const writeString = (view, offset, string) => {
            for (let i = 0; i < string.length; i++) {
                view.setUint8(offset + i, string.charCodeAt(i));
            }
        };
        
        // "RIFF" chunk descriptor
        writeString(view, 0, 'RIFF');
        view.setUint32(4, 36 + pcmData.length * 2, true);
        writeString(view, 8, 'WAVE');
        
        // "fmt " sub-chunk
        writeString(view, 12, 'fmt ');
        view.setUint32(16, 16, true);
        view.setUint16(20, 1, true);
        view.setUint16(22, 1, true);
        view.setUint32(24, this.WAV_SAMPLE_RATE, true);
        view.setUint32(28, this.WAV_SAMPLE_RATE * 2, true);
        view.setUint16(32, 2, true);
        view.setUint16(34, 16, true);
        
        // "data" sub-chunk
        writeString(view, 36, 'data');
        view.setUint32(40, pcmData.length * 2, true);

        // Combine header and data
        const wavArray = new Uint8Array(wavHeader.byteLength + pcmData.length * 2);
        wavArray.set(new Uint8Array(wavHeader), 0);
        wavArray.set(new Uint8Array(pcmData.buffer), wavHeader.byteLength);
        
        return wavArray;
    }

    async resumeRecording() {
        try {
            console.log('Resuming recording...');
            if (this.isRecording) {
                console.warn('Already recording');
                return;
            }

            // Request microphone access with specific constraints
            this.currentStream = await navigator.mediaDevices.getUserMedia({ 
                audio: {
                    sampleRate: this.WAV_SAMPLE_RATE,
                    channelCount: 1,
                    echoCancellation: true,
                    noiseSuppression: true,
                    autoGainControl: true
                }
            });
            
            // Create audio processing pipeline
            this.source = this.audioContext.createMediaStreamSource(this.currentStream);
            this.processor = this.audioContext.createScriptProcessor(4096, 1, 1);
            
            this.processor.onaudioprocess = (e) => {
                if (this.isRecording) {
                    const inputData = e.inputBuffer.getChannelData(0);
                    this.audioChunks.push(new Float32Array(inputData));
                    this.totalSamples += inputData.length;
                    
                    // Calculate current audio level
                    const audioLevel = Math.sqrt(inputData.reduce((acc, val) => acc + val * val, 0) / inputData.length);
                    
                    const currentTime = this.audioContext.currentTime;
                    const samplesFor30Seconds = this.WAV_SAMPLE_RATE * 30;
                    const recordingDuration = (Date.now() - this.recordingStartTime) / 1000;

                    // Only check for silence after minimum recording duration
                    if (recordingDuration >= this.MIN_RECORDING_DURATION) {
                        // Check for silence
                        if (audioLevel < this.SILENCE_THRESHOLD) {
                            this.consecutiveSilenceCount++;
                            if (this.silenceStartTime === 0) {
                                this.silenceStartTime = currentTime;
                                this.handleSilence();
                            }
                        } else {
                            this.consecutiveSilenceCount = 0;
                            this.silenceStartTime = 0;
                            if (this.autoStopTimeout) {
                                clearTimeout(this.autoStopTimeout);
                                this.autoStopTimeout = null;
                            }
                        }
                    }

                    // Process chunk if we have accumulated 30 seconds of audio
                    if (this.totalSamples >= samplesFor30Seconds) {
                        console.log('30-second duration reached, processing chunk...');
                        this.processCurrentChunk();
                        this.totalSamples = this.totalSamples % samplesFor30Seconds;
                    }

                    this.lastAudioLevel = audioLevel;
                }
            };
            
            this.source.connect(this.processor);
            this.processor.connect(this.audioContext.destination);
            
            this.isRecording = true;
            this.resumeButton.classList.add('hidden');
            this.recordButton.classList.remove('hidden');
            this.recordButton.textContent = 'Stop Recording';
            this.recordingStatus.textContent = 'Recording in progress...';
            this.recordingStatus.classList.add('text-red-500');

        } catch (error) {
            console.error('Error resuming recording:', error);
            this.showError('Could not resume recording. Please check microphone permissions.');
        }
    }

    async clearAll() {
        try {
            console.log('Clearing all data...');
            
            // Stop any ongoing recording
            if (this.isRecording) {
                await this.stopRecording();
            }

            // Clean up audio context and processing nodes
            if (this.source) {
                this.source.disconnect();
                this.source = null;
            }
            if (this.processor) {
                this.processor.disconnect();
                this.processor = null;
            }
            if (this.audioContext) {
                await this.audioContext.close();
                this.audioContext = null;
            }
            if (this.currentStream) {
                this.currentStream.getTracks().forEach(track => track.stop());
                this.currentStream = null;
            }
            
            // Clear text and audio data
            this.accumulatedText = '';
            this.temporaryText = '';
            this.transcriptionText.value = '';
            this.audioChunks = [];
            this.processingChunks = [];
            this.transcriptionQueue = [];
            this.isRecording = false;
            this.isProcessing = false;
            this.isProcessingQueue = false;
            
            // Reset UI
            this.recordingStatus.textContent = 'Click \'Start Recording\' to begin';
            this.clearButton.classList.add('hidden');
            this.resumeButton.classList.add('hidden');
            this.downloadMarkdown.disabled = true;
            this.downloadWord.disabled = true;
            this.downloadMarkdown.classList.add('opacity-50', 'cursor-not-allowed');
            this.downloadWord.classList.add('opacity-50', 'cursor-not-allowed');
            this.recordButton.textContent = 'Start Recording';
            
            // Re-initialize transcriber on the server
            await fetch('/transcribe_app/transcribe/', {
                method: 'POST',
                headers: {
                    'X-CSRFToken': getCSRFToken(),
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify({ action: 'reinitialize' })
            });
            
            console.log('All data cleared successfully');
        } catch (error) {
            console.error('Error clearing data:', error);
            this.showError('Failed to clear data');
        }
    }

    addToTranscriptionQueue(wavBlob) {
        this.transcriptionQueue.push(wavBlob);
        if (!this.isProcessingQueue) {
            this.processNextInQueue();
        }
    }

    showError(message) {
        console.error(message);
        this.recordingStatus.textContent = message;
        this.recordingStatus.classList.add('text-red-500');
    }

    getCsrfToken() {
        return document.querySelector('[name=csrfmiddlewaretoken]').value;
    }

    downloadTranscription(format) {
        const text = this.transcriptionText.value;
        if (!text) {
            this.showError('No transcription to download');
            return;
        }

        const blob = new Blob([text], { type: 'text/plain' });
        const url = window.URL.createObjectURL(blob);
        const a = document.createElement('a');
        a.href = url;
        a.download = `transcription.${format}`;
        document.body.appendChild(a);
        a.click();
        document.body.removeChild(a);
        window.URL.revokeObjectURL(url);
    }
}

// Initialize the audio transcription manager
const audioManager = new AudioTranscriptionManager();

// Add event listeners
document.getElementById('recordButton').addEventListener('click', async () => {
    if (!audioManager.isRecording) {
        await audioManager.startRecording();
    } else {
        await audioManager.stopRecording();
    }
});

document.getElementById('resumeButton').addEventListener('click', async () => {
    await audioManager.resumeRecording();
});

document.getElementById('downloadMarkdown').addEventListener('click', () => {
    audioManager.downloadTranscription('md');
});

document.getElementById('downloadWord').addEventListener('click', () => {
    audioManager.downloadTranscription('docx');
});

document.getElementById('clearButton').addEventListener('click', async () => {
    await audioManager.clearAll();
});

// Helper function to write string to DataView
function writeString(view, offset, string) {
    for (let i = 0; i < string.length; i++) {
        view.setUint8(offset + i, string.charCodeAt(i));
    }
}
</script>
{% endblock %}
